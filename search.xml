<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[论文思路mark(1)]]></title>
    <url>%2F2017%2F11%2F03%2F%E8%AE%BA%E6%96%87%E6%80%9D%E8%B7%AFmark%2F</url>
    <content type="text"><![CDATA[自编码网络 自编码网络（AutoEncoder），是一类输入完全等于输出的有监督神经网络。其特征为： 输入和输出的神经元个数完全相等 输出层与输入层存在关系 ： $y_k=x_k,k=1,2,...,n$ 隐层神经元个数小于输入（输出）神经元个数 其目的是：将高维的输入样本压缩到维度较小的隐藏层空间，以提取特征；并可通过输出层重建高维样本。类似于一个加密解密的过程。 多层感知器 多层感知器(Muti-Layer Percetron)是一类最简单也最常见的神经网络结构，多采用BP算法学习网络权重，以拟合输出层和输入层之间的关系。 假设样本集合有4维属性，用前三维属性拟合第四维属性，即求函数$f $，使得$x_4=f(x_1,x_2,x_3)$。求解时，可将$x_1,x_2,x_3$作为输入端，$x_4$作为输出端，构建网络架构。 自相关网络 在缺失数据填补领域，有一种思路是采用自相关网络+GA遗传算法来实现填补，其工作示意图如下： 个人观点，该方法存在以下问题： 自相关网络完全沿用了自编码器的思路，即“输入等于输出”。在网络训练时，由于期望输出信息完全包含于输入，BP算法会以一种偷懒的方式完成训练过程，即类似于使网络的输出和输入直接连接，而未真正训练出属性之间的非线性关系。 利用GA遗传算法求解目标函数最优解的前提是：网络训练效果非常好，并且网络已经挖掘出数据属性之间的真正联系。 针对上述第一个缺点，本文基于MTL思路，在自相关网络中，引入了回归任务，及通过 **”弱化跟踪，强化回归“**的方式，使得自相关网络避免偷懒，从而真正学习到属性之间的非线性关系。 改进网络结构描述 对于具有4列属性的样本集合，改进后的自相关网络示意图如下： 上图为4*（3+4）*4的网络结构，其中： 输入层和输出层的节点数均为4 隐藏层中，3个蓝色空心节点负责跟踪任务，4个蓝色实心节点负责回归任务 隐藏层激活函数为sigmod函数，输出层激活函数为线性函数 基于BP算法的权重更新推导 0.符号说明 符号 说明 $w_{ji}$ 输入层节点i到隐藏层节点j的连接权重 $b_{1i}$ 隐藏层节点j的阈值 $v_{kj}$ 隐藏层节点j到输出层节点k的连接权重 $b_{2i}$ 输出层节点j的阈值 1.跟踪任务（自相关）：(3个蓝色空心节点) 隐藏层输入为：$I(j)=\sum_{i=1}^{4}(w_{ji}x_{i})+b_{1j}$ 隐藏层输出为：$net(j)=\dfrac{1}{1+exp(-I(j))}$（sigmod函数） 输出层输入为：$O1(k)=\sum_{j=1}^{3}(v_{kj}*net(j))+b_{2k}$ 输出层输出为：$O1(k)$（线性函数） 2.回归任务：（4个蓝色实心节点） 此类节点不是传统意义上的隐藏层节点，针对不同的输出层节点，该隐层节点会有不同的输出结果。 针对输出层第一个节点，隐藏层输入、输出为：$$I(1，j)=\sum_{i=1,i \neq 1}^{4}(w_{ji}x_{i})+b_{1j}，net(1，j)=\dfrac{1}{1+exp(-I(1，j))}$$ 针对输出层第二个节点，隐藏层输入、输出为：$$I(2，j)=\sum_{i=1,i \neq 2}^{4}(w_{ji}x_{i})+b_{1j}，net(2，j)=\dfrac{1}{1+exp(-I(2，j))}$$ 针对输出层第三个节点，隐藏层输入、输出为：$$I(3，j)=\sum_{i=1,i \neq 3}^{4}(w_{ji}x_{i})+b_{1j}，net(3，j)=\dfrac{1}{1+exp(-I(3，j))}$$ 针对输出层第四个节点，隐藏层输入、输出为：$$I(4，j)=\sum_{i=1,i \neq 4}^{4}(w_{ji}x_{i})+b_{1j}，net(4，j)=\dfrac{1}{1+exp(-I(4，j))}$$ 输出层输入、输出：$O2(k)=\sum_{j=4}^{7}(v_{kj}*net(k,j))+b_{2k}$ 3.网络训练的目标函数 $$E=\sum_{k=1}^4\dfrac{1}{2}\alpha *(x_k-O1(k))^2+\dfrac{1}{2}(1-\alpha)(x_k-O2(k))^2$$ 4.网络权值更新 隐藏层到输出层的权重$v_{kj}、b_{2k}$的更新 $$\frac{\partial E}{\partial O1(k)}\frac{\partial O1(k)}{\partial v_{kj}}+\frac{\partial E}{\partial O2(k)}\frac{\partial O2(k)}{\partial v_{kj}} =-\sum_{k=1}^4[\alpha(x_k-O1(k))*net(j)+(1-\alpha)(x_k-O2(k))*net(k,j)]$$ $$ =-\sum_{k=1}^4 [\alpha(x_k-O1(k))*net(j)+(1-\alpha)(x_k-O2(k))*net(k,j)] $$ 同理： \begin{eqnarray*} \frac{\partial E}{\partial b_{2k}}&amp;=&amp;-\sum_{k=1}^4[\alpha(x_k-O1(k))+(1-\alpha)(x_k-O2(k))] \end{eqnarray*} 输入层到隐藏层的权重$w_{ji}、b_{1j}$的更新 $$ \frac{\partial E}{\partial w_{ji}} = \sum_{k=1}^4\frac{\partial E}{\partial O1(k)}\frac{\partial O1(k)}{\partial net(j)}\frac{\partial net(j)}{\partial I(j)}\frac{\partial I(j)}{\partial w_{ji}}+\frac{\partial E}{\partial O2(k)}\frac{\partial O2(k)}{\partial net(i,j)}\frac{\partial net(i,j)}{\partial I(i,j)}\frac{\partial I(i,j)}{\partial w_{ji}} $$ $$ = -\sum_{k=1}^4[\sum_{j=1}^3\alpha(x_k-O1(k))* v_{kj}* I(j)* (1-I(j))* x(i)+\sum_{j=4}^7(1-\alpha)(x_k-O2(k))* v_{kj}* I(k,j)* (1-I(k,j))* x(i)] $$ 同理： \begin{eqnarray*} \frac{\partial E}{\partial b_{1k}}&amp;=&amp;-\sum_{k=1}^4[\sum_{j=1}^3 \alpha(x_k-O1(k))* v_{kj}* I(j)* (1-I(j))+\sum_{j=4}^7 (1-\alpha)(x_k-O2(k))* v_{kj}* I(k,j)* (1-I(k,j))] \end{eqnarray*} 5.实验验证 制定实验计划中... 未完，待续]]></content>
      <categories>
        <category>研究工作</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>缺失数据填补</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我在想什么]]></title>
    <url>%2F2017%2F10%2F28%2F%E6%88%91%E5%9C%A8%E6%83%B3%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[愿： 善良的人永远被疼爱 努力的人永远有知己 一緒に、頑張ってね~ &lt;img src=&quot;http://img01.sogoucdn.com/app/a/100540002//1075607_s_90_2_219x160.jpg &quot; width=&quot;400&quot; height=&quot;400&quot; alt=&quot;&quot; /&gt;]]></content>
      <categories>
        <category>难得清闲</category>
      </categories>
  </entry>
</search>
